{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sea # Used to make prettier graphs\n",
    "\n",
    "# Load data\n",
    "X, Y = np.loadtxt(\"./pizza_data.txt\", skiprows=1, unpack=True)\n",
    "\n",
    "\n",
    "sea.set()\n",
    "# plt.axis([0, 50, 0 , 50])\n",
    "# plt.xticks(fontsize= 14)\n",
    "# plt.yticks(fontsize= 14)\n",
    "# plt.xlabel(\"Reservations\", fontsize=14)\n",
    "# plt.ylabel(\"Pizzas\", fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "## Quick reminder\n",
    "We are looking to minimize the loss in our algorithm. Imagine the loss function, graphed as a parabola. If we need to increase the weight to decrease the loss, we have a __negative__ slope for that point of the derivative of the graph. Vice versa for the opposite side of the parabola valley, where we need to decrease the weight to meet the minimum loss.\n",
    "\n",
    "## Math of gradient descent\n",
    "The following is the squared error loss:\n",
    "\n",
    "  L = (1/m) * Sum[i=0, i => m] of function ((w*xsubi + b ) - ysubi)^2\n",
    "\n",
    "The derivative of the function to calculate size and direction of the gradient is as follows:\n",
    "\n",
    "  (2/m) * Sum[i=0, i => m] of function xsubi * ((w*xsubi + b ) - ysubi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(X, w, b):\n",
    "  return X * w + b\n",
    "\n",
    "\n",
    "def loss(X, Y, w, b):\n",
    "  return np.average((predict(X, w, b) - Y) ** 2)\n",
    "\n",
    "\n",
    "# Our new gradient function, but currently b is set to 0\n",
    "def gradient(X, Y, w):\n",
    "  return 2 * np.average(X * (predict(X, w, 0) - Y))\n",
    "\n",
    "# Our new train function to do gradient descent\n",
    "# This will 'walk' the algorithm in the proper direction to minimize the loss without using case specific if statements.\n",
    "# Again, more iterations means closer to 0 loss, but akes much longer\n",
    "def train(X, Y, iterations, lr, verbose = False):\n",
    "  w = 0 \n",
    "  for i in range(iterations):\n",
    "    if(verbose):\n",
    "      print(\"Iteration %4d => Loss: %.10f\" % (i, loss(X, Y, w, 0)))\n",
    "\n",
    "    w -= gradient(X, Y, w) * lr\n",
    "\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weight = 1.8436928702\n"
     ]
    }
   ],
   "source": [
    "# Let's test out the new training function to find a good weight\n",
    "w = train(X, Y, 100, 0.001)\n",
    "print(\"\\nWeight = %.10f\" % w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
