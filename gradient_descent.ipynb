{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sea # Used to make prettier graphs\n",
    "\n",
    "# Load data\n",
    "X, Y = np.loadtxt(\"./pizza_data.txt\", skiprows=1, unpack=True)\n",
    "\n",
    "\n",
    "sea.set()\n",
    "# plt.axis([0, 50, 0 , 50])\n",
    "# plt.xticks(fontsize= 14)\n",
    "# plt.yticks(fontsize= 14)\n",
    "# plt.xlabel(\"Reservations\", fontsize=14)\n",
    "# plt.ylabel(\"Pizzas\", fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "## Quick reminder\n",
    "We are looking to minimize the loss in our algorithm. Imagine the loss function, graphed as a parabola. If we need to increase the weight to decrease the loss, we have a __negative__ slope for that point of the derivative of the graph. Vice versa for the opposite side of the parabola valley, where we need to decrease the weight to meet the minimum loss.\n",
    "\n",
    "## Math of gradient descent\n",
    "The following is the squared error loss:\n",
    "\n",
    "  L = (1/m) * Sum[i=0, i => m] of function ((w*xsubi + b ) - ysubi)^2\n",
    "\n",
    "The derivative of the function to calculate size and direction of the gradient is as follows:\n",
    "\n",
    "  (2/m) * Sum[i=0, i => m] of function xsubi * ((w*xsubi + b ) - ysubi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(X, w, b):\n",
    "  return X * w + b\n",
    "\n",
    "\n",
    "def loss(X, Y, w, b):\n",
    "  return np.average((predict(X, w, b) - Y) ** 2)\n",
    "\n",
    "\n",
    "# Our new gradient function, but currently b is set to 0\n",
    "def gradient(X, Y, w):\n",
    "  return 2 * np.average(X * (predict(X, w, 0) - Y))\n",
    "\n",
    "# Our new train function to do gradient descent\n",
    "# This will 'walk' the algorithm in the proper direction to minimize the loss without using case specific if statements.\n",
    "# Again, more iterations means closer to 0 loss, but akes much longer\n",
    "def train(X, Y, iterations, lr, verbose = False):\n",
    "  w = 0 \n",
    "  for i in range(iterations):\n",
    "    if(verbose):\n",
    "      print(\"Iteration %4d => Loss: %.10f\" % (i, loss(X, Y, w, 0)))\n",
    "\n",
    "    w -= gradient(X, Y, w) * lr\n",
    "\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weight = 1.8436928702\n"
     ]
    }
   ],
   "source": [
    "# Let's test out the new training function to find a good weight\n",
    "w = train(X, Y, 100, 0.001)\n",
    "print(\"\\nWeight = %.10f\" % w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at partial derivatives\n",
    "\n",
    "We originally had bias set as constant 0, but if we make it a variable, we're now adjusting two values (weight and bias) to minimize loss. Think of it as like trying to reach the lowest part in a valley, or a paper held by its corners at different heights. Essentially, we are trying to reach the minimum value for minimum loss.\n",
    "\n",
    "Gradient descent is still used, but will use a technique called partial derivatives.\n",
    "\n",
    "Partial derivatives are basically a slice of that valley / paper, the parabola that of a section but in 3d space. Remember the derivative of a cubed function is a squared function\n",
    "\n",
    "Basically, we're doing the gradient twice, once for b and once for w. Here's the formula for calculating b:\n",
    "(2/m) * Sum[i=0, i => m] of function ((w*xsubi + b ) - ysubi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "w = 1.0811301700, b = 13.1722676564\n",
      "Prediction: x = 20 reservations => y = 34.79 pizzas\n"
     ]
    }
   ],
   "source": [
    "def gradient_partial(X, Y, w, b):\n",
    "  w_gradient = 2 * np.average(X * (predict(X, w, b) - Y))\n",
    "  # We will now have a gradient for bias\n",
    "  b_gradient = 2 * np.average(predict(X, w, b) - Y)\n",
    "  return (w_gradient, b_gradient)\n",
    "\n",
    "def train_partial(X, Y, iterations, lr, verbose = False):\n",
    "  # bias will now be a variable instead of a constant\n",
    "  w = b = 0\n",
    "  for i in range(iterations):\n",
    "    if(verbose):\n",
    "      print(\"Iteration %4d => Loss %.10f\" % (i, loss(X, Y, w, b)))\n",
    "    w_gradient, b_gradient = gradient_partial(X, Y, w, b)\n",
    "    # Now we're updating all values every iteration\n",
    "    w -= w_gradient * lr\n",
    "    b -= b_gradient * lr\n",
    "  return w, b\n",
    "  \n",
    "# Testing it out with more iterations, as there's more that needs to be calculated\n",
    "reservations = 20\n",
    "w, b = train_partial(X, Y, 20000, 0.001)\n",
    "print(\"\\nw = %.10f, b = %.10f\" % (w, b))\n",
    "print(\"Prediction: x = %d reservations => y = %.2f pizzas\" % (reservations, predict(reservations, w, b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crazy, right? About a tenth more precise than the other function...\n",
    "I guess that's cool, but the function is way too much for something like predicting the amount of pizzas that will be sold.\n",
    "\n",
    "## Gradient Descent isn't end all, be all\n",
    "Gradient descent may not give us the optimal value, or could even miss the goal altogether. Think of the algorithm making some shape like sudden drops or local minimums being found instead of a global minimum (since the slope at a local minimum is also zero). The algorithm also uses a mean squared formula, which could also exaggerate other odd shapes.\n",
    "\n",
    "A good gradient descent function works so long as the shape being made is pretty smooth, no local minimums or gaps.\n",
    "\n",
    "## Other things to know\n",
    "Oversized learning rates can cause the algorithm to overshoot a loss correction. Think of a laser trying to reach the bottom of a cup using the reflections inside the cup. If the laser is too narrowly pointed, it might not even reach the minimum.\n",
    "\n",
    "\n",
    "**Let's move onto multiple linear regression**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
